{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project will create a data model for immigration data, and will build a data pipeline to perform the ETL process in spark which writes parquet file as output\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 0: Scope the Project\n",
    "* Step 1: Gather, Explore and Clean the Data\n",
    "* Step 2: Define the Data Model\n",
    "* Step 3: Run ETL to Model the Data\n",
    "* Step 4: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Scope the Project\n",
    "\n",
    "#### Scope \n",
    "##### - Data will be used: \n",
    "\n",
    "- immigration data: immigration_data_sample.csv\n",
    "    - Contains information about immigration details\n",
    "    - This data comes from the US National Tourism and Trade Office (https://travel.trade.gov/research/reports/i94/historical/2016.html).\n",
    "     \n",
    "     \n",
    "- i94 description file for immigrasion data: I94_SAS_Labels_Descriptions.sas\n",
    "     \n",
    "    - This is a file contains the descriptions and references for the codes being used in the immigration dataset. \n",
    "\n",
    "\n",
    "- demographic data: us-cities-demographics.csv\n",
    "    - Contains information about demographic data based on State-City pairs\n",
    "    - This data comes from OpenSoft (https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "     \n",
    "\n",
    "\n",
    "- downloaded dataset: /downloaded/FCDO_Geographical_Names_Index.csv\n",
    "    - Contains country name and 2-digit code, and other information about the country. This is to complement the i94 description data.\n",
    "    - This data comes from nationsonline.org (https://www.nationsonline.org/oneworld/country_code_list.htm).\n",
    "\n",
    "##### - End solution:\n",
    "\n",
    "A Data Model with a fact table that has immigration information and a series of dimension table with further details\n",
    "\n",
    "\n",
    "##### - Tools will be used:\n",
    "\n",
    " - One time pre-processing: Pandas\n",
    " - ETL pipeline: Spark\n",
    " - Storage: S3 bucket (this is not in the submitted version since it will take an extremely long time to write and read using S3. The submitted project only use local storage for storing the parquet files)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do all imports and installs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Gather, Explore and Clean the Data (This pre-processing step only needs to be ran the first time)\n",
    "#### Explore and clean the Data\n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "1. Immigration data needs more interpretability. Hence the i94 description file is introduced. In this file, below information will be extracted in this project:\n",
    "    - port\n",
    "    - visa\n",
    "    - mode\n",
    "    \n",
    "2. i94 file has limited informaiton about country data. A downloaded file contains 2-digit country name which will complement this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path explaining:\n",
    "\n",
    "- root path: resources from course\n",
    "\n",
    "- /downloaded: resources from online\n",
    "\n",
    "- /processed data: processed data combining different resources, which can be used in the project\n",
    "\n",
    "- /parquet files: output parquet data for data warehousing\n",
    "\n",
    "- /quality check: data needs to check and update\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed data path\n",
    "ppath='./processed data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Read Description File and Load to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file from SAS file description\n",
    "with open('./I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '')\n",
    "    \n",
    "def code_mapper(file, idx):\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94cit_res = code_mapper(f_content, \"i94cntyl\")\n",
    "i94port = code_mapper(f_content, \"i94prtl\")\n",
    "i94mode = code_mapper(f_content, \"i94model\")\n",
    "#i94addr = code_mapper(f_content, \"i94addrl\")\n",
    "i94visa = {'1':'Business',\n",
    "'2': 'Pleasure',\n",
    "'3' : 'Student'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cit_res_df=pd.DataFrame(list(i94cit_res.items()),columns=['Code','Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Code Descriptions from SAS file to Pandas DataFrame\n",
    "cit_res_df=pd.DataFrame(list(i94cit_res.items()),columns=['Code','Country'])\n",
    "port_df=pd.DataFrame(list(i94port.items()),columns=['Code','Address'])\n",
    "mode_df=pd.DataFrame(list(i94mode.items()),columns=['Code','Mode'])\n",
    "#addr_df=pd.DataFrame(list(i94addr.items()),columns=['Code','State'])\n",
    "visa_df=pd.DataFrame(list(i94visa.items()),columns=['Code','Visa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split City and State in Port address\n",
    "port_df['City']=port_df['Address'].apply(lambda x: x.split(',')[0] if len(x.split(','))==2 else x)\n",
    "port_df['City']=port_df['City'].apply(lambda x: ' '.join([i.lower().capitalize() for i in x.split()]))\n",
    "port_df['State']=port_df['Address'].apply(lambda x: x.split(',')[1] if len(x.split(','))==2 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the description to csv file for reference\n",
    "port_df.to_csv(ppath+'port.csv',index=False)\n",
    "mode_df.to_csv(ppath+'mode.csv',index=False)\n",
    "visa_df.to_csv(ppath+'visa.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Combine Downloaded Data and Description File for More Informative Country Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file from downloaded folder\n",
    "country = pd.read_csv('./downloaded/FCDO_Geographical_Names_Index.csv',encoding='ISO-8859-1')\n",
    "airport = pd.read_csv('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract continent information from airport data\n",
    "continent = airport[['continent','iso_country']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dataframes of i94, country and continent into one dataframe\n",
    "cit_res_df['Country_lower']=cit_res_df['Country'].apply(lambda x: x.lower())\n",
    "country['Country_lower']=country['Name'].apply(lambda x: x.lower())\n",
    "cit_res_df=cit_res_df.drop('Country',axis=1)\n",
    "country_df=pd.merge(cit_res_df,country,how='left',on='Country_lower')[['Code','Country','Name','Official name','Citizen names']]\n",
    "country_df=pd.merge(country_df,continent,how='left',left_on='Country', right_on='iso_country')[['Code','Country','Name','Official name','Citizen names','continent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df.to_csv(ppath+'country.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Data Model\n",
    "\n",
    "##### Fact table:\n",
    " - immigration table\n",
    "\n",
    "##### Dimension tables:\n",
    " - demographic table\n",
    " - state_race table\n",
    " - temperature table\n",
    " - port table\n",
    " - time table\n",
    " - country table\n",
    " - mode table\n",
    " - visa table\n",
    " \n",
    "#### 2.1 Assign Filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Files from course resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_file='immigration_data_sample.csv' # a subset of the immigration data\n",
    "#imm_file='../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat' # full dataset of the immigration data\n",
    "demo_file='us-cities-demographics.csv'\n",
    "temp_file='GlobalLandTemperaturesByCity.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Files for pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_file=ppath+'port.csv'\n",
    "mode_file=ppath+'mode.csv'\n",
    "visa_file=ppath+'visa.csv'\n",
    "country_file=ppath+'country.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Import Data and Map Out Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .load(imm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_imm = spark.read \\\n",
    "#        .format(\"com.github.saurfang.sas.spark\") \\\n",
    "#        .option(\"header\", \"true\") \\\n",
    "#        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "#        .load(imm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Function - Convert SAS date to datetime format\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def date_add_(date, days):\n",
    "\n",
    "    # Type check and convert to datetime object\n",
    "    if type(date) is not datetime:\n",
    "        date = datetime.strptime('1960-01-01', \"%Y-%m-%d\")\n",
    "        \n",
    "    # Convert non-numeric days to float\n",
    "    if type(days) is float:\n",
    "        return date + timedelta(days)\n",
    "    elif type(days) is not float and days is not None:\n",
    "        return date + timedelta(float(days))\n",
    "    \n",
    "    # Output null value for null value\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create udf to convert date fields from sas type to date type\n",
    "date_add_udf = f.udf(date_add_, t.DateType())\n",
    "# Add a column contains sas start date\n",
    "df_imm=df_imm.withColumn(\"sas_date\", lit('1960-01-01'))\n",
    "# Convert\n",
    "df_imm=df_imm.withColumn('arrdate_d', date_add_udf(f.to_date('sas_date'), 'arrdate'))\n",
    "df_imm=df_imm.withColumn('depdate_d', date_add_udf(f.to_date('sas_date'), 'depdate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df_imm=df_imm.withColumn('dtaddto_d',to_date(df_imm['dtaddto'],'mmddyyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp table for immigration data\n",
    "df_imm.createOrReplaceTempView(\"imm_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create immigration table - fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create immigration table - fact table\n",
    "immigration=spark.sql('''\n",
    "    SELECT DISTINCT \n",
    "    INT(cicid) as cicid,\n",
    "    INT(i94cit) AS cit_ctry,\n",
    "    INT(i94res) AS res_ctry,\n",
    "    i94port AS port_code,\n",
    "    INT(i94mode) AS trnps_mode_code,\n",
    "    i94addr AS address_state,\n",
    "    arrdate_d AS arrival_date,\n",
    "    depdate_d AS depart_date,\n",
    "    i94visa AS visa,\n",
    "    INT(i94bir) AS age,\n",
    "    INT(biryear) AS birth_year,\n",
    "    visapost AS dpmt_visa,\n",
    "    occup AS occupation,\n",
    "    dtaddto_d AS visa_expiry_date,\n",
    "    gender,\n",
    "    airline,\n",
    "    visatype AS visa_code\n",
    "    FROM imm_table\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.009009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.819958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count(1)\n",
       "count  111.000000\n",
       "mean     9.009009\n",
       "std     18.819958\n",
       "min      1.000000\n",
       "25%      1.000000\n",
       "50%      2.000000\n",
       "75%      9.000000\n",
       "max    128.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore for partition options\n",
    "spark.sql('''\n",
    "    select i94port,i94visa, count(*) from imm_table\n",
    "    group by i94port,i94visa''').toPandas().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time table - dimension table\n",
    "time=spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        arrdate_d AS date,\n",
    "        INT(YEAR(arrdate_d)) AS year,\n",
    "        INT(MONTH(arrdate_d)) AS month,\n",
    "        INT(DAY(arrdate_d)) AS day,\n",
    "        INT(WEEKDAY(arrdate_d)+1) AS weekday\n",
    "    FROM imm_table\n",
    "    WHERE arrdate_d is not null\n",
    "    UNION\n",
    "        SELECT DISTINCT\n",
    "        depdate_d AS date,\n",
    "        INT(YEAR(depdate_d)) AS year,\n",
    "        INT(MONTH(depdate_d)) AS month,\n",
    "        INT(DAY(depdate_d)) AS day,\n",
    "        INT(WEEKDAY(depdate_d)+1) AS weekday\n",
    "    FROM imm_table\n",
    "    WHERE depdate_d is not null\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .option(\"delimiter\",\";\") \\\n",
    "        .load(demo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.createOrReplaceTempView(\"demo_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create race table (granular level of demographic table) - dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create race table (granular level of demographic table) - dimension table\n",
    "state_race = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        City AS city,\n",
    "        State AS state,\n",
    "        `State Code` as state_code,\n",
    "        Race AS race,\n",
    "        int(Count) AS count,\n",
    "        ROW_NUMBER() OVER (PARTITION BY City, State ORDER BY int(Count) desc) order\n",
    "    FROM demo_table\n",
    "    ORDER BY City, State, Count desc\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>596.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.850671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.491456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count(1)\n",
       "count  596.000000\n",
       "mean     4.850671\n",
       "std      0.491456\n",
       "min      1.000000\n",
       "25%      5.000000\n",
       "50%      5.000000\n",
       "75%      5.000000\n",
       "max      5.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore partition options\n",
    "spark.sql('''\n",
    "    select city, state, count(*) from demo_table\n",
    "    group by city, state''').toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_race.createOrReplaceTempView(\"race_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create demographic table (city-state pair) - dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demographic table (city-state pair) - dimension table\n",
    "demographic = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        d.City AS city,\n",
    "        d.State AS state,\n",
    "        `State Code` as state_code,\n",
    "        INT(`Median Age`) AS median_age,\n",
    "        INT(`Male Population`) AS male_pplt,\n",
    "        INT(`Female Population`) AS female_pplt,\n",
    "        INT(`Total Population`) AS total_pplt,\n",
    "        INT(`Number of Veterans`) AS veteran_pplt,\n",
    "        INT(`Foreign-born`) AS foreign_born,\n",
    "        INT(`Average Household Size`) AS average_household_size,\n",
    "        r.race AS largest_race,\n",
    "        INT(r.count) AS largest_race_pplt\n",
    "    FROM demo_table d\n",
    "    JOIN race_table r on d.City = r.city and d.State = r.state and r.order = 1\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>state_code</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_pplt</th>\n",
       "      <th>female_pplt</th>\n",
       "      <th>total_pplt</th>\n",
       "      <th>veteran_pplt</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>average_household_size</th>\n",
       "      <th>largest_race</th>\n",
       "      <th>largest_race_pplt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sparks</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>NV</td>\n",
       "      <td>36</td>\n",
       "      <td>47780</td>\n",
       "      <td>48318</td>\n",
       "      <td>96098</td>\n",
       "      <td>7315</td>\n",
       "      <td>15690</td>\n",
       "      <td>2</td>\n",
       "      <td>White</td>\n",
       "      <td>78737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allen</td>\n",
       "      <td>Texas</td>\n",
       "      <td>TX</td>\n",
       "      <td>37</td>\n",
       "      <td>51324</td>\n",
       "      <td>46814</td>\n",
       "      <td>98138</td>\n",
       "      <td>3505</td>\n",
       "      <td>19649</td>\n",
       "      <td>3</td>\n",
       "      <td>White</td>\n",
       "      <td>69840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>IL</td>\n",
       "      <td>33</td>\n",
       "      <td>56229</td>\n",
       "      <td>62432</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634</td>\n",
       "      <td>7517</td>\n",
       "      <td>2</td>\n",
       "      <td>White</td>\n",
       "      <td>77074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kissimmee</td>\n",
       "      <td>Florida</td>\n",
       "      <td>FL</td>\n",
       "      <td>36</td>\n",
       "      <td>33283</td>\n",
       "      <td>35869</td>\n",
       "      <td>69152</td>\n",
       "      <td>2449</td>\n",
       "      <td>16879</td>\n",
       "      <td>3</td>\n",
       "      <td>White</td>\n",
       "      <td>51123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Danbury</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>CT</td>\n",
       "      <td>37</td>\n",
       "      <td>43435</td>\n",
       "      <td>41227</td>\n",
       "      <td>84662</td>\n",
       "      <td>3752</td>\n",
       "      <td>25675</td>\n",
       "      <td>2</td>\n",
       "      <td>White</td>\n",
       "      <td>55917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city        state state_code  median_age  male_pplt  female_pplt  \\\n",
       "0     Sparks       Nevada         NV          36      47780        48318   \n",
       "1      Allen        Texas         TX          37      51324        46814   \n",
       "2     Peoria     Illinois         IL          33      56229        62432   \n",
       "3  Kissimmee      Florida         FL          36      33283        35869   \n",
       "4    Danbury  Connecticut         CT          37      43435        41227   \n",
       "\n",
       "   total_pplt  veteran_pplt  foreign_born  average_household_size  \\\n",
       "0       96098          7315         15690                       2   \n",
       "1       98138          3505         19649                       3   \n",
       "2      118661          6634          7517                       2   \n",
       "3       69152          2449         16879                       3   \n",
       "4       84662          3752         25675                       2   \n",
       "\n",
       "  largest_race  largest_race_pplt  \n",
       "0        White              78737  \n",
       "1        White              69840  \n",
       "2        White              77074  \n",
       "3        White              51123  \n",
       "4        White              55917  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- median_age: integer (nullable = true)\n",
      " |-- male_pplt: integer (nullable = true)\n",
      " |-- female_pplt: integer (nullable = true)\n",
      " |-- total_pplt: integer (nullable = true)\n",
      " |-- veteran_pplt: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: integer (nullable = true)\n",
      " |-- largest_race: string (nullable = true)\n",
      " |-- largest_race_pplt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demographic.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.createOrReplaceTempView(\"temp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Create temperature table - dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temperature table - dimension table\n",
    "# Only collect data after 2004, and sort by date\n",
    "temperature=spark.sql('''\n",
    "    SELECT * FROM (\n",
    "        SELECT DISTINCT\n",
    "            dt AS date,\n",
    "            City AS city,\n",
    "            Country AS country,\n",
    "            AverageTemperature as avg_temperature,\n",
    "            AverageTemperatureUncertainty AS avg_temperature_uncertainty,\n",
    "            Latitude AS latitude,\n",
    "            Longitude AS longitude\n",
    "        FROM temp_table\n",
    "        WHERE YEAR(dt) between 2004 and 2013\n",
    "        ) AS a\n",
    "    ORDER BY date\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Other Ref Data for Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import country ref data\n",
    "# country table - dimension table\n",
    "df_country = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .load(country_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country.createOrReplaceTempView(\"country_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create country table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "country=spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        INT(Code) AS code,\n",
    "        Country AS country,\n",
    "        Name AS name,\n",
    "        `Official name` AS official_name,\n",
    "        `Citizen names` AS citizen_name,\n",
    "        continent\n",
    "    FROM country_table''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- official_name: string (nullable = true)\n",
      " |-- citizen_name: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create mode table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mode ref data\n",
    "# mode table - dimension table\n",
    "df_mode = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .load(mode_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mode.createOrReplaceTempView(\"mode_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=spark.sql('''\n",
    "    SELECT \n",
    "        INT(Code) AS code,\n",
    "        Mode AS mode\n",
    "    FROM mode_table''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create visa table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import visa ref data\n",
    "# visa table - dimension table\n",
    "df_visa = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .load(visa_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visa.createOrReplaceTempView(\"visa_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa=spark.sql('''\n",
    "    SELECT \n",
    "        INT(Code) AS code,\n",
    "        Visa AS visa\n",
    "    FROM visa_table''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- visa: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create port table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import port ref data\n",
    "# port table - dimension table\n",
    "df_port = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .load(port_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_port.createOrReplaceTempView(\"port_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "port=spark.sql('''\n",
    "    SELECT \n",
    "        Code AS code,\n",
    "        Address AS address,\n",
    "        City AS city,\n",
    "        State AS state\n",
    "    FROM port_table''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Pipeline\n",
    "\n",
    "#### 3.1 Write parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"parquet files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_data= os.path.join(output_data, \"immigration\")\n",
    "#immigration.write.parquet(imm_data, mode=\"overwrite\")\n",
    "immigration.write.partitionBy(\"port_code\",\"visa_code\").parquet(imm_data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data= os.path.join(output_data, \"demographic\")\n",
    "demographic.write.partitionBy(\"city\",\"state\").parquet(demo_data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data= os.path.join(output_data, \"time\")\n",
    "time.write.partitionBy(\"year\").parquet(time_data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data= os.path.join(output_data, \"state_race\")\n",
    "state_race.write.partitionBy(\"city\",\"state\").parquet(race_data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_data= os.path.join(output_data, \"temperature\")\n",
    "temperature.write.partitionBy(\"date\").parquet(temperature_data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data= os.path.join(output_data, \"country\")\n",
    "country.write.parquet(country_data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_data= os.path.join(output_data, \"port\")\n",
    "port.write.parquet(port_data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_data= os.path.join(output_data, \"mode\")\n",
    "mode.write.parquet(mode_data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa_data= os.path.join(output_data, \"visa\")\n",
    "visa.write.parquet(visa_data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Data Quality Checks\n",
    "\n",
    "Will check:\n",
    "1. Empty tables\n",
    "2. Missing values\n",
    "3. Duplicated keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign table names\n",
    "table_name = ['immigration','demographic','state_race','time','mode','visa','port','country','temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Check empty tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in table_name:\n",
    "    df_table=spark.read.parquet(output_data+\"/\"+table)\n",
    "    df_table.createOrReplaceTempView(table)\n",
    "    count_rows = spark.sql('''\n",
    "        SELECT count(*) FROM {}'''.format(table))\n",
    "    if count_rows.collect()[0][0] == 0:\n",
    "        raise ValueError(\"Data quality check failed. {} returned no results\").format(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "qpath=\"./quality check/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention required! 8 code do not have matched country information. Please update the reference data for country table.\n"
     ]
    }
   ],
   "source": [
    "# Checking no-matched country code in immigration data\n",
    "missing_country = spark.sql('''\n",
    "    SELECT i.country_code\n",
    "    FROM\n",
    "        (SELECT cit_ctry AS country_code\n",
    "        FROM immigration\n",
    "        UNION\n",
    "        SELECT res_ctry AS country_code\n",
    "        FROM immigration) i\n",
    "    LEFT JOIN country c on c.code = i.country_code\n",
    "    WHERE c.code IS NULL\n",
    "    ORDER BY c.code\n",
    "''')\n",
    "\n",
    "if missing_country.count() > 0:\n",
    "    missing_country.toPandas().to_csv(qpath+'missing_country_code.csv',index=False)\n",
    "    print(\"Attention required! {count} code do not have matched {name} information. Please update the reference data for {name} table.\" \\\n",
    "          .format(count=missing_country.count(),name=\"country\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking no-matched port code in immigration data\n",
    "missing_port = spark.sql('''\n",
    "    SELECT i.port_code\n",
    "    FROM immigration i\n",
    "    LEFT JOIN port p on p.code = i.port_code\n",
    "    WHERE p.code IS NULL\n",
    "    ORDER BY p.code\n",
    "''')\n",
    "\n",
    "if missing_port.count() > 0:\n",
    "    missing_port.toPandas().to_csv(qpath+'missing_{}_code.csv'.format(\"port\"),index=False)\n",
    "    print(\"Attention required! {count} code do not have matched {name} information. Please update the reference data for {name} table.\" \\\n",
    "          .format(count=missing_port.count(),name=\"port\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention required! 7 code do not have matched state information. Please update the reference data for state table.\n"
     ]
    }
   ],
   "source": [
    "# Checking no-matched State in immigration data\n",
    "missing_state = spark.sql('''\n",
    "    SELECT DISTINCT\n",
    "        i.address_state\n",
    "    FROM immigration i\n",
    "    LEFT JOIN demographic d on d.state_code = i.address_state\n",
    "    WHERE d.state_code IS NULL and i.address_state is not null\n",
    "    --ORDER BY d.state_code\n",
    "''')\n",
    "\n",
    "if missing_state.count() > 0:\n",
    "    missing_state.toPandas().to_csv(qpath+'missing_{}_code.csv'.format(\"state\"),index=False)\n",
    "    print(\"Attention required! {count} code do not have matched {name} information. Please update the reference data for {name} table.\" \\\n",
    "          .format(count=missing_state.count(),name=\"state\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.3 Checking duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if state code and state are one-to-one relationship in demo table\n",
    "dup_state = spark.sql('''\n",
    "    SELECT state_code, count(distinct state)\n",
    "    FROM demographic\n",
    "    group by state_code\n",
    "    having count(distinct state) >1\n",
    "''')\n",
    "\n",
    "if dup_state.count() > 0:\n",
    "    dup_state.toPandas().to_csv(qpath+'duplicated_{}.csv'.format(\"state\"),index=False)\n",
    "    print(\"Attention required! {count} code have duplicated {name} information. Please update the reference data for {name} table.\" \\\n",
    "          .format(count=dup_state.count(),name=\"state\"))\n",
    "    \n",
    "dup_state_code = spark.sql('''\n",
    "    SELECT state, count(distinct state_code)\n",
    "    FROM demographic\n",
    "    group by state\n",
    "    having count(distinct state_code) >1\n",
    "''')\n",
    "\n",
    "if dup_state_code.count() > 0:\n",
    "    dup_state_code.toPandas().to_csv(qpath+'duplicated_{}.csv'.format(\"state_code\"),index=False)\n",
    "    print(\"Attention required! {count} code have duplicated {name} information. Please update the reference data for {name} table.\" \\\n",
    "          .format(count=dup_state_code.count(),name=\"state_code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking duplicated city-state pair in demo table\n",
    "dup_demo = spark.sql('''\n",
    "    SELECT city, state\n",
    "    FROM demographic\n",
    "    group by city, state\n",
    "    having count(*) >1\n",
    "''')\n",
    "\n",
    "if dup_demo.count() > 0:\n",
    "    dup_demo.toPandas().to_csv(qpath+'duplicated_{}.csv'.format(\"city_state\"),index=False)\n",
    "    print(\"Attention required! {count} code have duplicated {name} information. Please update the reference data for {name} table.\" \\\n",
    "          .format(count=dup_demo.count(),name=\"city_state\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fact table:\n",
    "##### immigration\n",
    " - cicid: primary key, unique ID for each visiting record\n",
    " - cit_ctry: the country code the visitor's/immigrant's citizenship belongs to\n",
    " - res_ctry: the country code the visitor's/immigrant's residence belongs to\n",
    " - trnps_mode_code: the transport mode code for the visitor/immigrant coming to U.S.\n",
    " - address_state: visiting State\n",
    " - arrival_date: date of arrival at U.S.\n",
    " - depart_date: date of departure from U.S.\n",
    " - age: visitor's age\n",
    " - birth_year: visitor's birth year\n",
    " - dpmt_visa: department issued the visa\n",
    " - occupation: visitor/immigrant's occupation in U.S.\n",
    " - visa_expiry_date: visa's expiry date\n",
    " - gender: visitor's/immigrant's gender\n",
    " - airline: the airline company the visitor/immigrant took to U.S.\n",
    " - port_code: port information of landing\n",
    " - visa_type: type of visa was legally admitted for visiting U.S.\n",
    "\n",
    "#### Dimension table:\n",
    "##### demographic\n",
    "- city\n",
    "- state_code: 2-letter State code\n",
    "- state: state full name\n",
    "- median_age: median age within the city in the state\n",
    "- male_pplt: male population\n",
    "- female_pplt: female population\n",
    "- total_pplt: total population\n",
    "- veteran_pplt: the number of veteran\n",
    "- foreign_born: the number of foreign born\n",
    "- average_household_size: average house hold size in the city in the state\n",
    "- largest_race: largest race in the city in the state\n",
    "- largest_race_pplt: the population of the largest race in the city in the state\n",
    "        \n",
    "##### state_race: list all the races in each city in each state\n",
    "- city\n",
    "- state_code\n",
    "- state\n",
    "- race\n",
    "- count: population for the specific race\n",
    "- order: the descending order (from the largest to smallest) of the population for the specific race in the city in the state\n",
    "\n",
    "##### temperature: list the daily temperature for each city in each state for year 2004 ~ 2013\n",
    "- date\n",
    "- city\n",
    "- country\n",
    "- avg_temperature\n",
    "- avg_temperature_uncertainty\n",
    "- latitude\n",
    "- longitude\n",
    "\n",
    "##### time\n",
    "- date\n",
    "- month\n",
    "- day\n",
    "- weekday\n",
    "- year\n",
    "\n",
    "##### country\n",
    "- code: i94 country code, which is the code used in the immigration data\n",
    "- country: 2-letter country code\n",
    "- name: country name\n",
    "- official_name: the full name of the country\n",
    "- citizen_name: the citizen's name for that country\n",
    "- continent\n",
    "\n",
    "##### port\n",
    "- code: i94 port code, which is the code used in the immigration data\n",
    "- address: address contains city and state in the sas description file\n",
    "- city: parsed city\n",
    "- state: parsed state\n",
    "\n",
    "##### mode: transportation code\n",
    "- code: i94 transportation code being used in the immigration data\n",
    "- mode\n",
    "\n",
    "##### visa\n",
    "- code: i94 visa code being used in the immigration data\n",
    "- visa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Improvements needed:\n",
    "\n",
    "- Temperature table: has only city and country, but not state information. There are duplicated city names but within different states in U.S, and in fact, we have same city names but different state names in our dataset. In order to have better and more accurate analysis, state information needs to be added into the temperature table\n",
    "- Also, the temperature table needs to be updated for a couple of more years' data. The year as of this project right now is 2021, and we are missing almost 8 years data. The immigration data is based on 2016, so there was a big off as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>avg_temperature</th>\n",
       "      <th>avg_temperature_uncertainty</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beawar</td>\n",
       "      <td>India</td>\n",
       "      <td>19.804000000000002</td>\n",
       "      <td>0.513</td>\n",
       "      <td>26.52N</td>\n",
       "      <td>73.43E</td>\n",
       "      <td>2013-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Catania</td>\n",
       "      <td>Italy</td>\n",
       "      <td>7.625999999999999</td>\n",
       "      <td>0.379</td>\n",
       "      <td>37.78N</td>\n",
       "      <td>14.24E</td>\n",
       "      <td>2013-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hanover</td>\n",
       "      <td>Germany</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>0.274</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>10.51E</td>\n",
       "      <td>2013-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liancheng</td>\n",
       "      <td>China</td>\n",
       "      <td>18.04</td>\n",
       "      <td>0.603</td>\n",
       "      <td>21.70N</td>\n",
       "      <td>109.90E</td>\n",
       "      <td>2013-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Liverpool</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>4.6610000000000005</td>\n",
       "      <td>0.159</td>\n",
       "      <td>53.84N</td>\n",
       "      <td>4.09W</td>\n",
       "      <td>2013-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city         country     avg_temperature avg_temperature_uncertainty  \\\n",
       "0     Beawar           India  19.804000000000002                       0.513   \n",
       "1    Catania           Italy   7.625999999999999                       0.379   \n",
       "2    Hanover         Germany              -0.123                       0.274   \n",
       "3  Liancheng           China               18.04                       0.603   \n",
       "4  Liverpool  United Kingdom  4.6610000000000005                       0.159   \n",
       "\n",
       "  latitude longitude        date  \n",
       "0   26.52N    73.43E  2013-02-01  \n",
       "1   37.78N    14.24E  2013-02-01  \n",
       "2   52.24N    10.51E  2013-02-01  \n",
       "3   21.70N   109.90E  2013-02-01  \n",
       "4   53.84N     4.09W  2013-02-01  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up data in above tables\n",
    "spark.sql('''select * from temperature''').limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Complete Project Write Up\n",
    "##### 1. Clearly state the rationale for the choice of tools and technologies for the project, and propose how often the data should be updated and why.\n",
    "\n",
    "In this project, we mainly focus on the immigration data and related details around the immigration data. Some of data should be updated regularly, such as immigration data and demographic data as they are always dynamic. Some of the data can be considered as hard-coded data, such as the related i94 sas description data, which are references explaining some of the fields in immigration data. The reference data does not usually update very frequently, but should be monitored in a regular base so that any no-matched records can be detected and updated accordingly in a time manner.\n",
    "\n",
    "The retional of the data model design is to use python Pandas to pre-process the reference raw data and save them as hard-coded data/table, and we will maintain them as is moving forward. On the other hand, we use spark to load the dynamic data/table such as immigration and demographic data as well as the pre-processed the hard-coded reference data to the data model, which is so called, building the ETL pipeline. This ETL process will be performed regularly to keep the data warehouse almost always updated.\n",
    "\n",
    "\n",
    "##### 2. Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     - Re-design the partition for balanced seperation of the data and quicker retrieving\n",
    "     - Only update new records for immigration data instead of updating the entire table every time\n",
    "     - Design the data pipeline to include cloud computing, such as Redshift or S3 bucket, and use a suitable configuration for quicker uploads and downloads\n",
    "     \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     - Embed the data pipeline on Airflow to schedule the update of both the database and the dashboard\n",
    "     - Make sure no other big uploads/downloads for the machine at the same time to ensure quicker udpate and update successfully\n",
    "     \n",
    " * The database needed to be accessed by 100+ people.\n",
    "     - Uplaod the parquet data to private S3 bucket and grant access to the people who need to access the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. Analysis\n",
    "\n",
    "There are two separate files for further analysis based on this data model\n",
    "\n",
    "- Analysis in Pandas: Exploratory Analysis using Pandas, which was analyzing a subset of the dataset\n",
    "- Analysis in SQL: Exploratory Analysis using Spark SQL, which was analyzing the full dataset of the immigration data. This file has fewer analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
